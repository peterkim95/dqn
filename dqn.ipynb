{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69e12ece-5134-4ea8-bdf9-98b51b34dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aefec781-ae61-43bf-9bc3-457ba771683d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook dqn.ipynb to script\n",
      "[NbConvertApp] Writing 14231 bytes to dqn.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script dqn.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07e60eef-8233-4f93-94af-e889144b946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        # deque: Doubly Ended Queue\n",
    "    \n",
    "    def push(self, *args):\n",
    "        '''Save a transition'''\n",
    "        self.memory.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2e15f510-c025-46e0-955f-fd0561c68333",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7efea687-1e0a-407f-a2a7-3db982fc5384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = T.Compose([T.ToTensor(), T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "'''\n",
    "ToTensor():\n",
    "Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] \n",
    "'''\n",
    "\n",
    "def get_screen():\n",
    "    rgb_array = env.render(mode='rgb_array').copy()\n",
    "    assert rgb_array.shape == (400, 600, 3)\n",
    "    return transform(rgb_array).unsqueeze(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "807c90fc-177d-4c6c-980a-8f00ed82c4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 400, 600])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_screen().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd348770-c268-4879-a71f-3c4f011c78b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    '''\n",
    "    conv2d: (N, C, H, W) -> (N, C_out, H_out, W_out)\n",
    "    '''\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        def conv2d_size_out(size, kernel_size=5, stride=2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f2e2b609-f0b6-47a8-952b-7ff074d8752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "09236cdd-3360-4637-ab76-ae94121fbdf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7257, -0.2800]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.randn(1,3,400,600)\n",
    "policy_net(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e9136ac7-2e89-46ef-958e-66342e145931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net(i).max(1)[1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1148cbe0-82b4-4cd2-b07b-de2367f75b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp()\n",
    "    steps_done += 1\n",
    "    if random.random() >= eps_threshold:\n",
    "        return policy_net(i).max(1)[1].view(1,1) # Use policy net here...to decide best action so far\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b8dfb055-b772-4fb1-ad33-0cb085b530f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1689338-bd6d-4d30-82bc-36285ba2ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    batch = Transition(*zip(*transitions)) # concatenated along field axes\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3744f24e-73a5-4cdd-999e-6d4de67474b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.push(torch.randn(1,3,400,600), 0, torch.randn(1,3,400,600), 1)\n",
    "memory.push(torch.randn(1,3,400,600), 0, torch.randn(1,3,400,600), 1)\n",
    "memory.push(torch.randn(1,3,400,600), 0, torch.randn(1,3,400,600), 1)\n",
    "memory.push(torch.randn(1,3,400,600), 0, torch.randn(1,3,400,600), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1e2c754f-35c1-4b10-b4de-3e6907cb0d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = memory.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b7ee9c9b-10dc-4402-91de-ebe8e84894e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.9359, -0.3254,  1.1391,  ...,  1.3141,  0.6495,  0.6963],\n",
       "          [ 1.1139,  2.3185,  1.7724,  ...,  0.0150,  0.3312,  0.4021],\n",
       "          [ 0.4719, -0.0532,  0.8404,  ...,  1.9888,  0.6709,  0.5119],\n",
       "          ...,\n",
       "          [-0.5857,  0.1363, -0.2422,  ..., -1.0092, -1.4454,  0.2368],\n",
       "          [ 0.5230,  0.3253,  0.3292,  ..., -1.0470, -0.1470, -0.1809],\n",
       "          [-1.1985,  1.1009,  0.8172,  ...,  0.2116,  1.9131,  1.7149]],\n",
       "\n",
       "         [[-0.3002, -0.3937, -1.3914,  ...,  0.2192,  0.2968, -1.1763],\n",
       "          [-0.9238,  1.7163,  0.4355,  ...,  0.3918, -1.4436, -1.3221],\n",
       "          [-0.1748, -0.8825,  1.0331,  ...,  0.2434,  0.4167,  1.0066],\n",
       "          ...,\n",
       "          [-0.7874,  1.0651,  0.5141,  ..., -0.0892,  0.6730,  0.8772],\n",
       "          [ 0.3241, -0.4011,  0.3797,  ..., -0.0469, -0.0869,  0.1214],\n",
       "          [-1.1057,  0.5016,  1.2020,  ..., -0.3370,  0.5300, -0.7840]],\n",
       "\n",
       "         [[ 0.3869, -0.0559,  0.8213,  ..., -0.0308,  1.3288, -0.3785],\n",
       "          [-0.1517,  1.1028,  0.8536,  ..., -1.6411,  0.4678,  0.3755],\n",
       "          [-0.4992, -0.2079, -1.9693,  ...,  0.8092,  1.5526,  1.1515],\n",
       "          ...,\n",
       "          [-0.3184,  0.0869, -1.1272,  ..., -0.8243,  0.3447,  0.0148],\n",
       "          [-0.9173, -0.3462, -2.4500,  ...,  0.8106, -1.3065,  2.5492],\n",
       "          [ 1.4629, -1.7747,  0.8700,  ...,  0.5749,  0.7711, -0.5783]]]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Transition(*zip(*transitions)).next_state[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a702de7a-c326-46fd-8f4e-92aba3315544",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    \n",
    "    state = get_screen() - get_screen() # dummy state\n",
    "    t = 0\n",
    "    while True:\n",
    "        action = select_action(state)\n",
    "        \n",
    "        previous_screen = get_screen()\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        \n",
    "        if not done:\n",
    "            next_state = get_screen() - previous_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "        \n",
    "        memory.push(state, action, next_state, reward)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        optimize_model()\n",
    "        \n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "        \n",
    "        t += 1\n",
    "    \n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69b89f5-10b6-4182-ac59-4bc0aa5f4497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ab8e2-1ea9-4b8d-ba33-6a16f38d9079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f226b-868d-45d2-8c72-1d057a6fc4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43408964-70a3-43de-b295-6218afffe09e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9066c13e-44a6-4c0e-9a51-c5e779a048eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a197d27f-072a-43b3-9d12-52dada71bca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peter/miniconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00300323, -0.02719453,  0.00857147,  0.03419161])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0').unwrapped\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98edde8f-ccd0-4593-aa4c-fe024374bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac7ba8e-5d38-4751-8729-477e98419282",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71136c5c-14d3-4c75-8a6c-f4eb39a78858",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a25ba39-5ffa-409a-9823-0a542aabf9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        # deque: Doubly Ended Queue\n",
    "    \n",
    "    def push(self, *args):\n",
    "        '''Save a transition'''\n",
    "        self.memory.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61027a2a-41b7-46ce-89a9-0ac929b0f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        def conv2d_size_out(size, kernel_size=5, stride=2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9acf8a95-38dd-487b-966f-54e8e464bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2642e93-b2ab-4822-8b81-e91a4f1a053a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a737bb9-5f49-482f-ba4e-79fef1199b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b684ea-a1c3-40a7-84a5-458bc2b8a0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f97baa-0ff8-434c-a927-ec601c1a4284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peter/miniconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/Users/peter/miniconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdasd\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(300):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b4e63ab-ad1f-462e-91ad-5dc75bd740a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peter/miniconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.0387599 , -0.00171925,  0.02549799,  0.02399171])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b53195a0-ac66-42e1-bfe4-e1bcfb952a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "a = env.render(mode='rgb_array')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06474345-bf37-4580-81e9-49615cb04efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 600, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce909823-cd86-4e96-b31e-e1055c6c8ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03641905, 0.00716659, 0.00335491, 0.03700209])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5ef6d24-2c7c-42af-b7ba-cd54812b6ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abb2d5b4-771e-4444-ae2a-98447470b28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.03927384, -0.17772464, -0.04752022,  0.29644946]), 1.0, False, {})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08619290-9a97-4778-8c7a-1811df1b0656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(4,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99a4c96f-687e-4ede-84ec-7e9d8d508b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12ed3653-a5c2-46d6-8d8a-2093ae3a5bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.low)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "148d89c8-d755-4720-833c-d1202fcd4fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [position of cart, velocity of cart, angle of pole, rotation rate of pole]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7002350-edcd-48fb-a75a-3231f0f03637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spinningup",
   "language": "python",
   "name": "spinningup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
